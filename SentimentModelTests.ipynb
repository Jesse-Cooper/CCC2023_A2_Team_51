{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c0fd96-c838-43d1-88af-9b0e8f5965fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e59f912-7a90-4ccf-b301-c252f5fc6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = read_json_file('twitter-data-small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80df9e4b-0e81-4202-ad17-a829093f8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweets_by_keywords(tweets):\n",
    "    filtered_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if any(keyword in tweet['data']['text'].lower() for keyword in ['train', 'bus', 'tram', 'station']):\n",
    "            filtered_tweets.append(tweet)\n",
    "    return filtered_tweets\n",
    "\n",
    "def filter_tweets_by_location(tweets):\n",
    "    filtered_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if tweet['location'] is not None:\n",
    "            filtered_tweets.append(tweet)\n",
    "    return filtered_tweets\n",
    "\n",
    "filtered_tweets = filter_tweets_by_keywords(data)\n",
    "\n",
    "#filtered_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8c76cd8-dad9-4865-8537-d79f8818b9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.7712,\n",
       " 0.6404,\n",
       " 0.6124,\n",
       " -0.4826,\n",
       " 0.0,\n",
       " -0.5696,\n",
       " -0.941,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " -0.9136,\n",
       " -0.9501,\n",
       " -0.743,\n",
       " 0.0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def get_sentiment_score(tweet):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    scores = analyzer.polarity_scores(tweet)\n",
    "    sentiment_score = scores['compound']\n",
    "    return sentiment_score\n",
    "\n",
    "def get_sentiment_list(filtered_tweets):\n",
    "    sentiment = []\n",
    "    for i in range(len(filtered_tweets)):\n",
    "        sentiment.append(get_sentiment_score(filtered_tweets[i]['data']['text']))\n",
    "    return sentiment\n",
    "    \n",
    "sentiment_list = get_sentiment_list(filtered_tweets)\n",
    "#sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2f7c57d-eab9-49b2-ba70-8f94e59179c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to categorise in positive/negative\n",
    "def sentiment_category(sentiment_list):\n",
    "    result = []\n",
    "    for sen in sentiment_list:\n",
    "        if sen <= -0.4:\n",
    "            result.append(\"negative\")\n",
    "        elif sen >= 0.4:\n",
    "            result.append(\"positive\")\n",
    "        else:\n",
    "            result.append(\"neutral\")\n",
    "    return result\n",
    "\n",
    "sentiment_category = sentiment_category(sentiment_list)\n",
    "#sentiment_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87b0ffd7-c1aa-4e2f-8cc3-56684dac1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map with sentiment\n",
    "\n",
    "def plot_sentiment_on_map(data):\n",
    "    vic_map = folium.Map(location=[-37.4713, 144.7852], zoom_start=8)\n",
    "    for tweet in data:\n",
    "        if tweet['place']:\n",
    "            place = tweet['place']\n",
    "            if place['country_code'] == 'AU' and place['place_type'] == 'city':\n",
    "                lat = place['bounding_box']['coordinates'][0][0][1]\n",
    "                lon = place['bounding_box']['coordinates'][0][0][0]\n",
    "                name = place['name']\n",
    "                sentiment_score = get_sentiment_score(tweet['text'])\n",
    "                if sentiment_score > 0.5:\n",
    "                    color = 'green'\n",
    "                elif sentiment_score > 0:\n",
    "                    color = 'lightgreen'\n",
    "                elif sentiment_score > -0.5:\n",
    "                    color = 'yellow'\n",
    "                else:\n",
    "                    color = 'red'\n",
    "                folium.CircleMarker([lat, lon], radius=5, color=color, fill=True, fill_opacity=0.7, popup=name + ': ' + str(sentiment_score)).add_to(vic_map)\n",
    "    return vic_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d0b8b-a5d7-4d32-8c44-7ac1d3b67e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toot_sentiment(instance_url, access_token, #toot):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    #not sure if needed?\n",
    "    mastodon = Mastodon(\n",
    "        access_token=access_token,\n",
    "        api_base_url=instance_url\n",
    "    )\n",
    "    for toot in timeline:\n",
    "        #regex\n",
    "        clean_toot = re.sub('<[^<]+?>', '', toot['content'])\n",
    "        tokens = word_tokenize(clean_toot)\n",
    "        scores = analyzer.polarity_scores(clean_toot)\n",
    "        sentiment_score = scores['compound']\n",
    "        return sentiment_score, clean_toot\n",
    "\n",
    "#map with sentiment\n",
    "def plot_sentiment_on_map(data):\n",
    "    vic_map = folium.Map(location=[-37.4713, 144.7852], zoom_start=8)\n",
    "    for tweet in data:\n",
    "        if tweet['place']:\n",
    "            place = tweet['place']\n",
    "            if place['country_code'] == 'AU' and place['place_type'] == 'city':\n",
    "                lat = place['bounding_box']['coordinates'][0][0][1]\n",
    "                lon = place['bounding_box']['coordinates'][0][0][0]\n",
    "                name = place['name']\n",
    "                sentiment_score = get_sentiment_score(tweet['text'])\n",
    "                if sentiment_score > 0.5:\n",
    "                    color = 'green'\n",
    "                elif sentiment_score > 0:\n",
    "                    color = 'lightgreen'\n",
    "                elif sentiment_score > -0.5:\n",
    "                    color = 'yellow'\n",
    "                else:\n",
    "                    color = 'red'\n",
    "                folium.CircleMarker([lat, lon], radius=5, color=color, fill=True, fill_opacity=0.7, popup=name + ': ' + str(sentiment_score)).add_to(vic_map)\n",
    "    return vic_map\n",
    "\n",
    "#compare twitter and mastadon sentiment based on location\n",
    "def compare_sentiment_location():\n",
    "    twitter_scores_by_location = {}\n",
    "    toot_scores_by_location = {}\n",
    "    #twitter\n",
    "    twitter_tweets = get_twitter_sentiment_location(twitter_api_key, twitter_api_secret_key, twitter_access_token, twitter_access_token_secret)\n",
    "    for tweet in twitter_tweets:\n",
    "        location = tweet['location']\n",
    "        sentiment = tweet['sentiment']\n",
    "        if location in twitter_scores_by_location:\n",
    "            twitter_scores_by_location[location].append(sentiment)\n",
    "        else:\n",
    "            twitter_scores_by_location[location] = [sentiment]\n",
    "    #toots\n",
    "    toot_statuses = get_toot_sentiment_location(instance_url, access_token)\n",
    "    for status in toot_statuses:\n",
    "        location = status['location']\n",
    "        sentiment = status['sentiment']\n",
    "        if location in toot_scores_by_location:\n",
    "            toot_scores_by_location[location].append(sentiment)\n",
    "        else:\n",
    "            toot_scores_by_location[location] = [sentiment]\n",
    "    \n",
    "    #mean for each\n",
    "    twitter_means_by_location = {}\n",
    "    for location, scores in twitter_scores_by_location.items():\n",
    "        mean_score = statistics.mean(scores)\n",
    "        twitter_means_by_location[location] = mean_score\n",
    "        \n",
    "    toot_means_by_location = {}\n",
    "    for location, scores in toot_scores_by_location.items():\n",
    "        mean_score = statistics.mean(scores)\n",
    "        toot_means_by_location[location] = mean_score\n",
    "    \n",
    "    #comparison\n",
    "    for location in set(twitter_means_by_location.keys()) | set(toot_means_by_location.keys()):\n",
    "        twitter_mean = twitter_means_by_location.get(location, 0)\n",
    "        toot_mean = toot_means_by_location.get(location, 0)\n",
    "        if twitter_mean > toot_mean:\n",
    "            print(f\"{location}: Twitter has a higher mean sentiment score than Mastodon.\")\n",
    "        elif toot_mean > twitter_mean:\n",
    "            print(f\"{location}: Mastodon has a higher mean sentiment score than Twitter.\")\n",
    "        else:\n",
    "            print(f\"{location}: The mean sentiment scores for Twitter and Mastodon are the same.\")\n",
    "                       \n",
    "def process_data(twitter_data_iter, sal_data):\n",
    "    authors = []\n",
    "    city_tweets = defaultdict(int)\n",
    "    unique_city_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for tweet in twitter_data_iter:\n",
    "        author_id = tweet['data']['author_id']\n",
    "        authors.append(author_id)\n",
    "        \n",
    "        full_name = tweet['includes']['places'][0]['full_name']\n",
    "        city = full_name.split(',')[0].strip().lower()\n",
    "        \n",
    "        if city in sal_data:\n",
    "            city_code = sal_data[city]['gcc']\n",
    "            city_tweets[city_code] += 1\n",
    "            unique_city_tweets[author_id][city_code] += 1\n",
    "\n",
    "    return city_tweets, unique_city_tweets\n",
    "\n",
    "#for scaling\n",
    "def get_file_chunks(filename, num_chunks):\n",
    "    file_size = os.path.getsize(filename)\n",
    "    chunk_size = file_size // num_chunks\n",
    "    return [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_chunks)]\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    num_procs = comm.Get_size()\n",
    "\n",
    "    sal_data = read_json_file('sal.json')\n",
    "\n",
    "    # Calculate the number of lines in the file\n",
    "    with open('bigTwitter.json', 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    # Calculate the lines each process should read\n",
    "    lines_per_proc = total_lines // num_procs\n",
    "    start_line = rank * lines_per_proc\n",
    "    end_line = start_line + lines_per_proc if rank != num_procs - 1 else total_lines\n",
    "\n",
    "    twitter_data_iter = read_json_file_iteratively('#######.json')\n",
    "\n",
    "\n",
    "    # TBC - what we actually want to do, easy with functions\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
