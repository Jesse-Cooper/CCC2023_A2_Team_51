{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bab8ea-25cb-4b94-b50d-d80ab2fdf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import couchdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52826d6-8458-4edf-ac94-3510edb6c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"http://admin:admin@172.26.136.58:5984\"\n",
    "import couchdb\n",
    "import json\n",
    "couch = couchdb.Server(HOST)\n",
    "db = couch['test_twitter2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfda95-c958-496b-a700-161186e55f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ids\n",
    "i = []\n",
    "count = 0\n",
    "for docid in db.view('_all_docs'):\n",
    "    i.append(docid['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993bdbf-05d4-43d1-ba16-efaad723aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc_id in i:\n",
    "    if not doc_id.startswith('_'):  # Skip special documents\n",
    "        doc = db[doc_id]\n",
    "        location = doc.get('location')\n",
    "        text = doc.get('text')\n",
    "        if location and text: \n",
    "            data.append({'location': location, 'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd995c3-5e37-4278-a596-ce417db6064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data2.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda8f57-314c-4502-8029-7dcb846d0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def get_sentiment_score(tweet):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    scores = analyzer.polarity_scores(tweet)\n",
    "    sentiment_score = scores['compound']\n",
    "    return sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6f14b-cbd0-40a1-aaba-716d6ef242b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_list(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]['sentiment'] = get_sentiment_score(data[i]['text'])\n",
    "        #sentiment.append(get_sentiment_score(filtered_tweets[i]['data']['text']))\n",
    "get_sentiment_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fe0fa-94e1-4965-964b-4e4fb5a390c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('data2.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "filtered_data = []\n",
    "desired_words = ['bus ', 'tram ', 'train ', 'ptv']\n",
    "\n",
    "for item in data:\n",
    "    tweet = item.get('text', '')  # Get the value of the 'tweet' element (or use a default empty string if not present)\n",
    "    if any(word in tweet for word in desired_words):\n",
    "        filtered_data.append(item)\n",
    "\n",
    "with open('filtered.json', 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
